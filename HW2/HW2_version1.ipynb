{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6791d003220b4b65956097c8a4c6f9ae",
    "deepnote_cell_type": "code",
    "execution_context_id": "f5831625-1df5-4e2f-8f38-79330cac3e8c",
    "execution_millis": 9964,
    "execution_start": 1764324392899,
    "source_hash": "6a74492e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from time import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from msc_dataset import MSCDataset\n",
    "\n",
    "# For data augmentation\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup for Mac M4 Pro (MPS), CUDA (NVIDIA), or CPU fallback\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "2bf1095ce3714193a5704972d8eabc47",
    "deepnote_cell_type": "code",
    "execution_context_id": "f5831625-1df5-4e2f-8f38-79330cac3e8c",
    "execution_millis": 0,
    "execution_start": 1764324402909,
    "source_hash": "2bb072ee"
   },
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "CFG = {\n",
    "    'sampling_rate': 16000,\n",
    "    'frame_length_in_s': 0.04,\n",
    "    'frame_step_in_s': 0.02,\n",
    "    'n_mels': 40,\n",
    "    'f_min': 20,  # Ottimizzato per voce umana\n",
    "    'f_max': 4000,\n",
    "    'seed': 42,\n",
    "    'train_steps': 5000,  # Pi√π step per epoch\n",
    "    'train_batch_size': 64,  # Batch pi√π piccolo per better convergence\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 50,  # Pi√π epochs con early stopping\n",
    "    # Data Augmentation\n",
    "    'time_shift_ms': 100,  # shift audio di ¬±100ms\n",
    "    'noise_level': 0.005,   # background noise\n",
    "    'time_stretch_factor': 0.1,  # speed variation\n",
    "}\n",
    "\n",
    "# Define the set of target classes\n",
    "CLASSES = ['stop', 'up']\n",
    "\n",
    "# Set Deterministic Behaviour\n",
    "torch.manual_seed(CFG['seed'])\n",
    "np.random.seed(CFG['seed'])\n",
    "random.seed(CFG['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "ecbf3afbfac942669a020b568e53f6e7",
    "deepnote_cell_type": "code",
    "execution_context_id": "f5831625-1df5-4e2f-8f38-79330cac3e8c",
    "execution_millis": 0,
    "execution_start": 1764324402959,
    "source_hash": "ec07639e"
   },
   "outputs": [],
   "source": [
    "# ==================== MEL-SPECTROGRAM FEATURE EXTRACTOR ====================\n",
    "class MelSpectrogramExtractor(nn.Module):\n",
    "    \"\"\"ONNX-compatible Mel-Spectrogram feature extractor\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=CFG['sampling_rate'],\n",
    "            n_fft=int(CFG['frame_length_in_s'] * CFG['sampling_rate']),\n",
    "            hop_length=int(CFG['frame_step_in_s'] * CFG['sampling_rate']),\n",
    "            n_mels=CFG['n_mels'],\n",
    "            f_min=CFG['f_min'],\n",
    "            f_max=CFG['f_max'],\n",
    "            window_fn=torch.hann_window,\n",
    "            power=2.0,\n",
    "            normalized=False,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\"\n",
    "        )\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        # waveform: (batch, samples)\n",
    "        mel_spec = self.mel_transform(waveform)  # (batch, n_mels, time)\n",
    "        \n",
    "        # Log scale\n",
    "        log_mel = torch.log(mel_spec + 1e-9)\n",
    "        \n",
    "        # Normalize to [-1, 1] range (per-sample)\n",
    "        log_mel = (log_mel - log_mel.mean(dim=[1, 2], keepdim=True)) / (log_mel.std(dim=[1, 2], keepdim=True) + 1e-9)\n",
    "        \n",
    "        return log_mel.unsqueeze(1)  # (batch, 1, n_mels, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DATA AUGMENTATION ====================\n",
    "class AudioAugmentation:\n",
    "    \"\"\"Data augmentation per audio waveforms\"\"\"\n",
    "    def __init__(self, config, training=True):\n",
    "        self.config = config\n",
    "        self.training = training\n",
    "        \n",
    "    def time_shift(self, waveform):\n",
    "        \"\"\"Shift audio randomly in time\"\"\"\n",
    "        if not self.training or random.random() > 0.5:\n",
    "            return waveform\n",
    "            \n",
    "        shift_samples = int(random.uniform(-self.config['time_shift_ms'], \n",
    "                                          self.config['time_shift_ms']) \n",
    "                          * self.config['sampling_rate'] / 1000)\n",
    "        return torch.roll(waveform, shifts=shift_samples, dims=-1)\n",
    "    \n",
    "    def add_noise(self, waveform):\n",
    "        \"\"\"Add background white noise\"\"\"\n",
    "        if not self.training or random.random() > 0.5:\n",
    "            return waveform\n",
    "            \n",
    "        noise = torch.randn_like(waveform) * self.config['noise_level']\n",
    "        return waveform + noise\n",
    "    \n",
    "    def time_stretch(self, waveform):\n",
    "        \"\"\"Slightly speed up or slow down audio\"\"\"\n",
    "        if not self.training or random.random() > 0.5:\n",
    "            return waveform\n",
    "            \n",
    "        rate = 1.0 + random.uniform(-self.config['time_stretch_factor'], \n",
    "                                    self.config['time_stretch_factor'])\n",
    "        \n",
    "        # Simple resampling-based time stretch\n",
    "        stretched = F.interpolate(\n",
    "            waveform.unsqueeze(0), \n",
    "            size=int(waveform.shape[-1] * rate),\n",
    "            mode='linear',\n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        # Pad or crop to original length\n",
    "        target_len = waveform.shape[-1]\n",
    "        if stretched.shape[-1] < target_len:\n",
    "            stretched = F.pad(stretched, (0, target_len - stretched.shape[-1]))\n",
    "        else:\n",
    "            stretched = stretched[..., :target_len]\n",
    "            \n",
    "        return stretched\n",
    "    \n",
    "    def __call__(self, waveform):\n",
    "        \"\"\"Apply all augmentations in sequence\"\"\"\n",
    "        waveform = self.time_shift(waveform)\n",
    "        waveform = self.add_noise(waveform)\n",
    "        waveform = self.time_stretch(waveform)\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "69e8d27312ad462980b748e61e3b30c5",
    "deepnote_cell_type": "code",
    "execution_context_id": "f5831625-1df5-4e2f-8f38-79330cac3e8c",
    "execution_millis": 1,
    "execution_start": 1764324403019,
    "source_hash": "688087c5"
   },
   "outputs": [],
   "source": [
    "# ==================== CNN MODEL (IMPROVED) ====================\n",
    "class KeywordSpotter(nn.Module):\n",
    "    \"\"\"Enhanced CNN for Up/Stop classification with Dropout\"\"\"\n",
    "    def __init__(self, num_classes=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: 1 ‚Üí 64 channels\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout2d(dropout * 0.5)  # Light dropout early\n",
    "        \n",
    "        # Block 2: 64 ‚Üí 64 channels, downsample\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout2d(dropout * 0.5)\n",
    "        \n",
    "        # Block 3: 64 ‚Üí 128 channels, downsample\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.dropout3 = nn.Dropout2d(dropout)\n",
    "        \n",
    "        # Block 4: 128 ‚Üí 128 channels\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Block 5: 128 ‚Üí 256 channels (NEW)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.dropout5 = nn.Dropout2d(dropout)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Classifier with dropout\n",
    "        self.dropout_fc = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(256, num_classes, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, 40, 49)\n",
    "        x = self.dropout1(self.relu1(self.bn1(self.conv1(x))))  # (batch, 64, 40, 49)\n",
    "        x = self.dropout2(self.relu2(self.bn2(self.conv2(x))))  # (batch, 64, 20, 25)\n",
    "        x = self.dropout3(self.relu3(self.bn3(self.conv3(x))))  # (batch, 128, 10, 13)\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))                 # (batch, 128, 10, 13)\n",
    "        x = self.dropout5(self.relu5(self.bn5(self.conv5(x))))  # (batch, 256, 10, 13)\n",
    "        \n",
    "        x = self.gap(x)  # (batch, 256, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (batch, 256)\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc(x)  # (batch, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "921d386640ed4102b50d9506b5682916",
    "deepnote_cell_type": "code",
    "execution_context_id": "f5831625-1df5-4e2f-8f38-79330cac3e8c",
    "execution_millis": 41,
    "execution_start": 1764324403069,
    "source_hash": "5b3f3265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "UP/STOP KEYWORD SPOTTER - TRAINING PIPELINE (ENHANCED)\n",
      "============================================================\n",
      "Device: mps\n",
      "Mel-Spectrogram config: n_mels=40, n_fft=640, hop=320\n",
      "Training config: epochs=50, batch_size=64, lr=0.001\n",
      "Data Augmentation: time_shift=¬±100ms, noise=0.005, stretch=¬±10.0%\n",
      "============================================================\n",
      "\n",
      "üìÅ Loading datasets...\n",
      "Using data folder: ./msc-training\n",
      "Loaded 1600 samples from ./msc-training for classes ['stop', 'up']\n",
      "Using data folder: ./msc-validation\n",
      "Loaded 200 samples from ./msc-validation for classes ['stop', 'up']\n",
      "Using data folder: ./msc-testing\n",
      "Loaded 200 samples from ./msc-testing for classes ['stop', 'up']\n",
      "\n",
      "üèóÔ∏è  Initializing models...\n",
      "Model parameters: 555,330\n",
      "Estimated size (float32): 2169.26 KB\n",
      "Estimated size (int8): 542.31 KB\n"
     ]
    }
   ],
   "source": [
    "# ==================== MAIN TRAINING PIPELINE ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"UP/STOP KEYWORD SPOTTER - TRAINING PIPELINE (ENHANCED)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Mel-Spectrogram config: n_mels={CFG['n_mels']}, n_fft={int(CFG['frame_length_in_s'] * CFG['sampling_rate'])}, hop={int(CFG['frame_step_in_s'] * CFG['sampling_rate'])}\")\n",
    "print(f\"Training config: epochs={CFG['epochs']}, batch_size={CFG['train_batch_size']}, lr={CFG['learning_rate']}\")\n",
    "print(f\"Data Augmentation: time_shift=¬±{CFG['time_shift_ms']}ms, noise={CFG['noise_level']}, stretch=¬±{CFG['time_stretch_factor']*100}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create Mel-Spectrogram transform\n",
    "transform = MelSpectrogramExtractor()\n",
    "\n",
    "# Create augmentation\n",
    "train_augmentation = AudioAugmentation(CFG, training=True)\n",
    "val_augmentation = AudioAugmentation(CFG, training=False)\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nüìÅ Loading datasets...\")\n",
    "train_dataset = MSCDataset(\n",
    "    root='.',\n",
    "    classes=CLASSES,\n",
    "    split='training',\n",
    "    preprocess=None,\n",
    ")\n",
    "\n",
    "val_dataset = MSCDataset(\n",
    "    root='.',\n",
    "    classes=CLASSES,\n",
    "    split='validation',\n",
    "    preprocess=None,\n",
    ")\n",
    "\n",
    "test_dataset = MSCDataset(\n",
    "    root='.',\n",
    "    classes=CLASSES,\n",
    "    split='testing',\n",
    "    preprocess=None,\n",
    ")\n",
    "\n",
    "# Create dataloaders with RandomSampler for training\n",
    "sampler = torch.utils.data.RandomSampler(\n",
    "    train_dataset,\n",
    "    replacement=True,\n",
    "    num_samples=CFG['train_steps'] * CFG['train_batch_size'],\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CFG['train_batch_size'],\n",
    "    sampler=sampler,\n",
    "    num_workers=0,  # Set to 0 for macOS compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, num_workers=0)\n",
    "\n",
    "# Initialize models\n",
    "print(\"\\nüèóÔ∏è  Initializing models...\")\n",
    "feature_extractor = MelSpectrogramExtractor().to(DEVICE)\n",
    "model = KeywordSpotter(num_classes=len(CLASSES), dropout=0.3).to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(f\"Estimated size (float32): {total_params * 4 / 1024:.2f} KB\")\n",
    "print(f\"Estimated size (int8): {total_params / 1024:.2f} KB\")\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "# Scheduler - More gradual decay\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping setup\n",
    "best_val_acc = 0\n",
    "best_val_loss = float('inf')\n",
    "patience = 15  # More patience\n",
    "patience_counter = 0\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAINING & EVALUATION FUNCTIONS ====================\n",
    "\n",
    "def evaluate(model, feature_extractor, loader, device):\n",
    "    \"\"\"Evaluate model on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    total_loss = 0\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch['x'].squeeze(1).to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            features = feature_extractor(x)\n",
    "            output = model(features)\n",
    "            predictions = output.argmax(dim=1)\n",
    "            correct += (predictions == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            total_loss += loss_module(output, y).item()\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def train_epoch(model, feature_extractor, train_loader, optimizer, loss_module, device, \n",
    "                steps_per_epoch, current_epoch, augmentation):\n",
    "    \"\"\"Train model for one epoch with data augmentation\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    start_step = current_epoch * steps_per_epoch\n",
    "    end_step = start_step + steps_per_epoch\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if step < start_step:\n",
    "            continue\n",
    "        if step >= end_step:\n",
    "            break\n",
    "            \n",
    "        x = batch['x'].squeeze(1)  # Keep on CPU for augmentation\n",
    "        y = batch['y'].to(device)\n",
    "        \n",
    "        # Apply data augmentation on CPU\n",
    "        x_augmented = []\n",
    "        for i in range(x.shape[0]):\n",
    "            aug_sample = augmentation(x[i:i+1])\n",
    "            x_augmented.append(aug_sample)\n",
    "        x = torch.cat(x_augmented, dim=0).to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(x)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(features)\n",
    "        loss = loss_module(output, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        step_count += 1\n",
    "        \n",
    "        if ((step + 1) % 100) == 0 or step == 0:\n",
    "            print(f'Step={step}; Training Loss={loss.item():.4f}')\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / step_count if step_count > 0 else 0\n",
    "    return avg_epoch_loss\n",
    "\n",
    "\n",
    "def test_model(model, feature_extractor, test_loader, device):\n",
    "    \"\"\"Test model on test dataset\"\"\"\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch['x'].squeeze(1).to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            features = feature_extractor(x)\n",
    "            \n",
    "            output = model(features)\n",
    "            predictions = output.argmax(dim=1)\n",
    "            \n",
    "            correct += (predictions == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training...\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/50\n",
      "============================================================\n",
      "Step=0; Training Loss=0.7418\n",
      "Step=0; Training Loss=0.7418\n",
      "Step=99; Training Loss=0.4684\n",
      "\n",
      "üìä Epoch 1 Summary:\n",
      "   Train Loss: 0.5945\n",
      "   Val Acc: 78.00%\n",
      "   Val Loss: 0.4290\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=78.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/50\n",
      "============================================================\n",
      "Step=99; Training Loss=0.4684\n",
      "\n",
      "üìä Epoch 1 Summary:\n",
      "   Train Loss: 0.5945\n",
      "   Val Acc: 78.00%\n",
      "   Val Loss: 0.4290\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=78.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 2/50\n",
      "============================================================\n",
      "Step=199; Training Loss=0.4439\n",
      "\n",
      "üìä Epoch 2 Summary:\n",
      "   Train Loss: 0.4131\n",
      "   Val Acc: 88.00%\n",
      "   Val Loss: 0.2403\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=88.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/50\n",
      "============================================================\n",
      "Step=199; Training Loss=0.4439\n",
      "\n",
      "üìä Epoch 2 Summary:\n",
      "   Train Loss: 0.4131\n",
      "   Val Acc: 88.00%\n",
      "   Val Loss: 0.2403\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=88.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 3/50\n",
      "============================================================\n",
      "Step=299; Training Loss=0.3500\n",
      "\n",
      "üìä Epoch 3 Summary:\n",
      "   Train Loss: 0.3266\n",
      "   Val Acc: 92.50%\n",
      "   Val Loss: 0.2239\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=92.50%\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/50\n",
      "============================================================\n",
      "Step=299; Training Loss=0.3500\n",
      "\n",
      "üìä Epoch 3 Summary:\n",
      "   Train Loss: 0.3266\n",
      "   Val Acc: 92.50%\n",
      "   Val Loss: 0.2239\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=92.50%\n",
      "\n",
      "============================================================\n",
      "EPOCH 4/50\n",
      "============================================================\n",
      "Step=399; Training Loss=0.2535\n",
      "\n",
      "üìä Epoch 4 Summary:\n",
      "   Train Loss: 0.2856\n",
      "   Val Acc: 93.00%\n",
      "   Val Loss: 0.1602\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=93.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/50\n",
      "============================================================\n",
      "Step=399; Training Loss=0.2535\n",
      "\n",
      "üìä Epoch 4 Summary:\n",
      "   Train Loss: 0.2856\n",
      "   Val Acc: 93.00%\n",
      "   Val Loss: 0.1602\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=93.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 5/50\n",
      "============================================================\n",
      "Step=499; Training Loss=0.1880\n",
      "\n",
      "üìä Epoch 5 Summary:\n",
      "   Train Loss: 0.2338\n",
      "   Val Acc: 93.00%\n",
      "   Val Loss: 0.1528\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 1/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 6/50\n",
      "============================================================\n",
      "Step=499; Training Loss=0.1880\n",
      "\n",
      "üìä Epoch 5 Summary:\n",
      "   Train Loss: 0.2338\n",
      "   Val Acc: 93.00%\n",
      "   Val Loss: 0.1528\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 1/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 6/50\n",
      "============================================================\n",
      "Step=599; Training Loss=0.2035\n",
      "\n",
      "üìä Epoch 6 Summary:\n",
      "   Train Loss: 0.1929\n",
      "   Val Acc: 91.50%\n",
      "   Val Loss: 0.1999\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 2/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 7/50\n",
      "============================================================\n",
      "Step=599; Training Loss=0.2035\n",
      "\n",
      "üìä Epoch 6 Summary:\n",
      "   Train Loss: 0.1929\n",
      "   Val Acc: 91.50%\n",
      "   Val Loss: 0.1999\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 2/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 7/50\n",
      "============================================================\n",
      "Step=699; Training Loss=0.2126\n",
      "\n",
      "üìä Epoch 7 Summary:\n",
      "   Train Loss: 0.1618\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.1075\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=97.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 8/50\n",
      "============================================================\n",
      "Step=699; Training Loss=0.2126\n",
      "\n",
      "üìä Epoch 7 Summary:\n",
      "   Train Loss: 0.1618\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.1075\n",
      "   Learning Rate: 0.001000\n",
      "‚úÖ New best model! Val Acc=97.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 8/50\n",
      "============================================================\n",
      "Step=799; Training Loss=0.0720\n",
      "\n",
      "üìä Epoch 8 Summary:\n",
      "   Train Loss: 0.1280\n",
      "   Val Acc: 95.50%\n",
      "   Val Loss: 0.1383\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 1/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 9/50\n",
      "============================================================\n",
      "Step=799; Training Loss=0.0720\n",
      "\n",
      "üìä Epoch 8 Summary:\n",
      "   Train Loss: 0.1280\n",
      "   Val Acc: 95.50%\n",
      "   Val Loss: 0.1383\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 1/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 9/50\n",
      "============================================================\n",
      "Step=899; Training Loss=0.0981\n",
      "\n",
      "üìä Epoch 9 Summary:\n",
      "   Train Loss: 0.1078\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.1206\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 2/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 10/50\n",
      "============================================================\n",
      "Step=899; Training Loss=0.0981\n",
      "\n",
      "üìä Epoch 9 Summary:\n",
      "   Train Loss: 0.1078\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.1206\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 2/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 10/50\n",
      "============================================================\n",
      "Step=999; Training Loss=0.0990\n",
      "\n",
      "üìä Epoch 10 Summary:\n",
      "   Train Loss: 0.1011\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0846\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 3/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 11/50\n",
      "============================================================\n",
      "Step=999; Training Loss=0.0990\n",
      "\n",
      "üìä Epoch 10 Summary:\n",
      "   Train Loss: 0.1011\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0846\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 3/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 11/50\n",
      "============================================================\n",
      "Step=1099; Training Loss=0.1345\n",
      "\n",
      "üìä Epoch 11 Summary:\n",
      "   Train Loss: 0.0956\n",
      "   Val Acc: 95.50%\n",
      "   Val Loss: 0.1126\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 4/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 12/50\n",
      "============================================================\n",
      "Step=1099; Training Loss=0.1345\n",
      "\n",
      "üìä Epoch 11 Summary:\n",
      "   Train Loss: 0.0956\n",
      "   Val Acc: 95.50%\n",
      "   Val Loss: 0.1126\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 4/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 12/50\n",
      "============================================================\n",
      "Step=1199; Training Loss=0.0319\n",
      "\n",
      "üìä Epoch 12 Summary:\n",
      "   Train Loss: 0.0862\n",
      "   Val Acc: 94.00%\n",
      "   Val Loss: 0.1335\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 5/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 13/50\n",
      "============================================================\n",
      "Step=1199; Training Loss=0.0319\n",
      "\n",
      "üìä Epoch 12 Summary:\n",
      "   Train Loss: 0.0862\n",
      "   Val Acc: 94.00%\n",
      "   Val Loss: 0.1335\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 5/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 13/50\n",
      "============================================================\n",
      "Step=1299; Training Loss=0.0339\n",
      "\n",
      "üìä Epoch 13 Summary:\n",
      "   Train Loss: 0.0847\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0931\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 6/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 14/50\n",
      "============================================================\n",
      "Step=1299; Training Loss=0.0339\n",
      "\n",
      "üìä Epoch 13 Summary:\n",
      "   Train Loss: 0.0847\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0931\n",
      "   Learning Rate: 0.001000\n",
      "‚è≥ No improvement. Patience: 6/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 14/50\n",
      "============================================================\n",
      "Step=1399; Training Loss=0.0315\n",
      "\n",
      "üìä Epoch 14 Summary:\n",
      "   Train Loss: 0.0685\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0920\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 7/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 15/50\n",
      "============================================================\n",
      "Step=1399; Training Loss=0.0315\n",
      "\n",
      "üìä Epoch 14 Summary:\n",
      "   Train Loss: 0.0685\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0920\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 7/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 15/50\n",
      "============================================================\n",
      "Step=1499; Training Loss=0.0174\n",
      "\n",
      "üìä Epoch 15 Summary:\n",
      "   Train Loss: 0.0615\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0957\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 8/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 16/50\n",
      "============================================================\n",
      "Step=1499; Training Loss=0.0174\n",
      "\n",
      "üìä Epoch 15 Summary:\n",
      "   Train Loss: 0.0615\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0957\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 8/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 16/50\n",
      "============================================================\n",
      "Step=1599; Training Loss=0.0221\n",
      "\n",
      "üìä Epoch 16 Summary:\n",
      "   Train Loss: 0.0607\n",
      "   Val Acc: 98.00%\n",
      "   Val Loss: 0.0962\n",
      "   Learning Rate: 0.000500\n",
      "‚úÖ New best model! Val Acc=98.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 17/50\n",
      "============================================================\n",
      "Step=1599; Training Loss=0.0221\n",
      "\n",
      "üìä Epoch 16 Summary:\n",
      "   Train Loss: 0.0607\n",
      "   Val Acc: 98.00%\n",
      "   Val Loss: 0.0962\n",
      "   Learning Rate: 0.000500\n",
      "‚úÖ New best model! Val Acc=98.00%\n",
      "\n",
      "============================================================\n",
      "EPOCH 17/50\n",
      "============================================================\n",
      "Step=1699; Training Loss=0.0613\n",
      "\n",
      "üìä Epoch 17 Summary:\n",
      "   Train Loss: 0.0566\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0871\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 1/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 18/50\n",
      "============================================================\n",
      "Step=1699; Training Loss=0.0613\n",
      "\n",
      "üìä Epoch 17 Summary:\n",
      "   Train Loss: 0.0566\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0871\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 1/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 18/50\n",
      "============================================================\n",
      "Step=1799; Training Loss=0.1429\n",
      "\n",
      "üìä Epoch 18 Summary:\n",
      "   Train Loss: 0.0577\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.1119\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 2/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 19/50\n",
      "============================================================\n",
      "Step=1799; Training Loss=0.1429\n",
      "\n",
      "üìä Epoch 18 Summary:\n",
      "   Train Loss: 0.0577\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.1119\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 2/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 19/50\n",
      "============================================================\n",
      "Step=1899; Training Loss=0.0340\n",
      "\n",
      "üìä Epoch 19 Summary:\n",
      "   Train Loss: 0.0537\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.0862\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 3/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 20/50\n",
      "============================================================\n",
      "Step=1899; Training Loss=0.0340\n",
      "\n",
      "üìä Epoch 19 Summary:\n",
      "   Train Loss: 0.0537\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.0862\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 3/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 20/50\n",
      "============================================================\n",
      "Step=1999; Training Loss=0.1020\n",
      "\n",
      "üìä Epoch 20 Summary:\n",
      "   Train Loss: 0.0465\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.1178\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 4/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 21/50\n",
      "============================================================\n",
      "Step=1999; Training Loss=0.1020\n",
      "\n",
      "üìä Epoch 20 Summary:\n",
      "   Train Loss: 0.0465\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.1178\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 4/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 21/50\n",
      "============================================================\n",
      "Step=2099; Training Loss=0.0774\n",
      "\n",
      "üìä Epoch 21 Summary:\n",
      "   Train Loss: 0.0482\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0882\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 5/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 22/50\n",
      "============================================================\n",
      "Step=2099; Training Loss=0.0774\n",
      "\n",
      "üìä Epoch 21 Summary:\n",
      "   Train Loss: 0.0482\n",
      "   Val Acc: 96.50%\n",
      "   Val Loss: 0.0882\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 5/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 22/50\n",
      "============================================================\n",
      "Step=2199; Training Loss=0.0539\n",
      "\n",
      "üìä Epoch 22 Summary:\n",
      "   Train Loss: 0.0498\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0933\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 6/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 23/50\n",
      "============================================================\n",
      "Step=2199; Training Loss=0.0539\n",
      "\n",
      "üìä Epoch 22 Summary:\n",
      "   Train Loss: 0.0498\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0933\n",
      "   Learning Rate: 0.000500\n",
      "‚è≥ No improvement. Patience: 6/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 23/50\n",
      "============================================================\n",
      "Step=2299; Training Loss=0.0245\n",
      "\n",
      "üìä Epoch 23 Summary:\n",
      "   Train Loss: 0.0493\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0894\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 7/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 24/50\n",
      "============================================================\n",
      "Step=2299; Training Loss=0.0245\n",
      "\n",
      "üìä Epoch 23 Summary:\n",
      "   Train Loss: 0.0493\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0894\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 7/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 24/50\n",
      "============================================================\n",
      "Step=2399; Training Loss=0.0252\n",
      "\n",
      "üìä Epoch 24 Summary:\n",
      "   Train Loss: 0.0401\n",
      "   Val Acc: 95.00%\n",
      "   Val Loss: 0.1125\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 8/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 25/50\n",
      "============================================================\n",
      "Step=2399; Training Loss=0.0252\n",
      "\n",
      "üìä Epoch 24 Summary:\n",
      "   Train Loss: 0.0401\n",
      "   Val Acc: 95.00%\n",
      "   Val Loss: 0.1125\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 8/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 25/50\n",
      "============================================================\n",
      "Step=2499; Training Loss=0.0187\n",
      "\n",
      "üìä Epoch 25 Summary:\n",
      "   Train Loss: 0.0378\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0885\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 9/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 26/50\n",
      "============================================================\n",
      "Step=2499; Training Loss=0.0187\n",
      "\n",
      "üìä Epoch 25 Summary:\n",
      "   Train Loss: 0.0378\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0885\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 9/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 26/50\n",
      "============================================================\n",
      "Step=2599; Training Loss=0.0212\n",
      "\n",
      "üìä Epoch 26 Summary:\n",
      "   Train Loss: 0.0415\n",
      "   Val Acc: 98.00%\n",
      "   Val Loss: 0.0978\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 10/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 27/50\n",
      "============================================================\n",
      "Step=2599; Training Loss=0.0212\n",
      "\n",
      "üìä Epoch 26 Summary:\n",
      "   Train Loss: 0.0415\n",
      "   Val Acc: 98.00%\n",
      "   Val Loss: 0.0978\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 10/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 27/50\n",
      "============================================================\n",
      "Step=2699; Training Loss=0.0472\n",
      "\n",
      "üìä Epoch 27 Summary:\n",
      "   Train Loss: 0.0394\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0930\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 11/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 28/50\n",
      "============================================================\n",
      "Step=2699; Training Loss=0.0472\n",
      "\n",
      "üìä Epoch 27 Summary:\n",
      "   Train Loss: 0.0394\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0930\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 11/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 28/50\n",
      "============================================================\n",
      "Step=2799; Training Loss=0.0163\n",
      "\n",
      "üìä Epoch 28 Summary:\n",
      "   Train Loss: 0.0395\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.1104\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 12/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 29/50\n",
      "============================================================\n",
      "Step=2799; Training Loss=0.0163\n",
      "\n",
      "üìä Epoch 28 Summary:\n",
      "   Train Loss: 0.0395\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.1104\n",
      "   Learning Rate: 0.000250\n",
      "‚è≥ No improvement. Patience: 12/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 29/50\n",
      "============================================================\n",
      "Step=2899; Training Loss=0.0161\n",
      "\n",
      "üìä Epoch 29 Summary:\n",
      "   Train Loss: 0.0337\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0961\n",
      "   Learning Rate: 0.000125\n",
      "‚è≥ No improvement. Patience: 13/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 30/50\n",
      "============================================================\n",
      "Step=2899; Training Loss=0.0161\n",
      "\n",
      "üìä Epoch 29 Summary:\n",
      "   Train Loss: 0.0337\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0961\n",
      "   Learning Rate: 0.000125\n",
      "‚è≥ No improvement. Patience: 13/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 30/50\n",
      "============================================================\n",
      "Step=2999; Training Loss=0.0064\n",
      "\n",
      "üìä Epoch 30 Summary:\n",
      "   Train Loss: 0.0324\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0962\n",
      "   Learning Rate: 0.000125\n",
      "‚è≥ No improvement. Patience: 14/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 31/50\n",
      "============================================================\n",
      "Step=2999; Training Loss=0.0064\n",
      "\n",
      "üìä Epoch 30 Summary:\n",
      "   Train Loss: 0.0324\n",
      "   Val Acc: 97.00%\n",
      "   Val Loss: 0.0962\n",
      "   Learning Rate: 0.000125\n",
      "‚è≥ No improvement. Patience: 14/15\n",
      "\n",
      "============================================================\n",
      "EPOCH 31/50\n",
      "============================================================\n",
      "Step=3099; Training Loss=0.0216\n",
      "\n",
      "üìä Epoch 31 Summary:\n",
      "   Train Loss: 0.0337\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.1041\n",
      "   Learning Rate: 0.000125\n",
      "‚è≥ No improvement. Patience: 15/15\n",
      "\n",
      "üõë Early stopping at epoch 31\n",
      "\n",
      "‚úÖ Loaded best model with Val Acc=98.00%\n",
      "\n",
      "============================================================\n",
      "Training completed after 31 epochs\n",
      "Best validation accuracy: 98.00%\n",
      "============================================================\n",
      "Step=3099; Training Loss=0.0216\n",
      "\n",
      "üìä Epoch 31 Summary:\n",
      "   Train Loss: 0.0337\n",
      "   Val Acc: 97.50%\n",
      "   Val Loss: 0.1041\n",
      "   Learning Rate: 0.000125\n",
      "‚è≥ No improvement. Patience: 15/15\n",
      "\n",
      "üõë Early stopping at epoch 31\n",
      "\n",
      "‚úÖ Loaded best model with Val Acc=98.00%\n",
      "\n",
      "============================================================\n",
      "Training completed after 31 epochs\n",
      "Best validation accuracy: 98.00%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRAINING LOOP ====================\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "\n",
    "steps_per_epoch = len(train_loader) // CFG['epochs']\n",
    "current_epoch = 0\n",
    "\n",
    "train_history = {'epoch': [], 'train_loss': [], 'val_acc': [], 'val_loss': [], 'lr': []}\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch+1}/{CFG['epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(\n",
    "        model, feature_extractor, train_loader, optimizer, loss_module, \n",
    "        DEVICE, steps_per_epoch, epoch, train_augmentation\n",
    "    )\n",
    "    \n",
    "    current_epoch += 1\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_acc, val_loss = evaluate(model, feature_extractor, val_loader, DEVICE)\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f'\\nüìä Epoch {current_epoch} Summary:')\n",
    "    print(f'   Train Loss: {train_loss:.4f}')\n",
    "    print(f'   Val Acc: {val_acc:.2f}%')\n",
    "    print(f'   Val Loss: {val_loss:.4f}')\n",
    "    print(f'   Learning Rate: {current_lr:.6f}')\n",
    "    \n",
    "    # Save history\n",
    "    train_history['epoch'].append(current_epoch)\n",
    "    train_history['train_loss'].append(train_loss)\n",
    "    train_history['val_acc'].append(val_acc)\n",
    "    train_history['val_loss'].append(val_loss)\n",
    "    train_history['lr'].append(current_lr)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(f'‚úÖ New best model! Val Acc={val_acc:.2f}%')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'‚è≥ No improvement. Patience: {patience_counter}/{patience}')\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'\\nüõë Early stopping at epoch {current_epoch}')\n",
    "        break\n",
    "    \n",
    "    # Learning rate scheduler step (ReduceLROnPlateau needs metric)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f'\\n‚úÖ Loaded best model with Val Acc={best_val_acc:.2f}%')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training completed after {current_epoch} epochs\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating model on test set...\n",
      "\n",
      "üéØ Test Accuracy: 99.00%\n",
      "‚ùå FAILED: Accuracy <= 99.4%\n"
     ]
    }
   ],
   "source": [
    "# ==================== TEST EVALUATION ====================\n",
    "print(\"\\nüìä Evaluating model on test set...\")\n",
    "\n",
    "test_accuracy = test_model(model, feature_extractor, test_loader, DEVICE)\n",
    "print(f'\\nüéØ Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "if test_accuracy > 99.4:\n",
    "    print(\"‚úÖ PASSED: Accuracy > 99.4%\")\n",
    "else:\n",
    "    print(\"‚ùå FAILED: Accuracy <= 99.4%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "99a6df57d92146afae7054ec23e004f7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MODEL\n",
      "============================================================\n",
      "Model Timestamp: 1764345312\n",
      "\n",
      "üîÑ Moving models to CPU for ONNX export...\n",
      "\n",
      "üì¶ Exporting Feature Extractor to ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 16:55:12.471000 36687 torch/onnx/_internal/exporter/_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MelSpectrogramExtractor([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MelSpectrogramExtractor([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 16:55:12.911000 36687 torch/onnx/_internal/exporter/_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ‚úÖ\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
      "Applied 4 of general pattern rewrite rules.\n",
      "‚úÖ Feature extractor saved: ./saved_models//1764345312_frontend.onnx\n",
      "\n",
      "üì¶ Exporting Model to ONNX...\n",
      "[torch.onnx] Obtain model graph for `KeywordSpotter([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `KeywordSpotter([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ‚úÖ\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
      "Applied 10 of general pattern rewrite rules.\n",
      "‚úÖ Model saved: ./saved_models//1764345312_model.onnx\n",
      "\n",
      "============================================================\n",
      "SIZE REPORT (ONNX - Float32)\n",
      "============================================================\n",
      "Feature Extractor: 332.49 KB\n",
      "Model: 2178.31 KB\n",
      "Total: 2510.80 KB\n",
      "‚ö†Ô∏è  WARNING: Size > 300 KB - quantization required!\n",
      "\n",
      "üìù Saving hyperparameters and results...\n",
      "‚úÖ Results saved to ./keyword_spotter_results.csv\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "[torch.onnx] Obtain model graph for `KeywordSpotter([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ‚úÖ\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
      "Applied 10 of general pattern rewrite rules.\n",
      "‚úÖ Model saved: ./saved_models//1764345312_model.onnx\n",
      "\n",
      "============================================================\n",
      "SIZE REPORT (ONNX - Float32)\n",
      "============================================================\n",
      "Feature Extractor: 332.49 KB\n",
      "Model: 2178.31 KB\n",
      "Total: 2510.80 KB\n",
      "‚ö†Ô∏è  WARNING: Size > 300 KB - quantization required!\n",
      "\n",
      "üìù Saving hyperparameters and results...\n",
      "‚úÖ Results saved to ./keyword_spotter_results.csv\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================== SAVE MODEL ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timestamp = int(time())\n",
    "saved_model_dir = './saved_models/'\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "\n",
    "print(f'Model Timestamp: {timestamp}')\n",
    "\n",
    "model.eval()\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Move models to CPU for ONNX export (MPS not supported for export)\n",
    "print(\"\\nüîÑ Moving models to CPU for ONNX export...\")\n",
    "model_cpu = model.cpu()\n",
    "feature_extractor_cpu = feature_extractor.cpu()\n",
    "\n",
    "# Export Feature Extractor to ONNX\n",
    "print(\"\\nüì¶ Exporting Feature Extractor to ONNX...\")\n",
    "torch.onnx.export(\n",
    "    feature_extractor_cpu,  # model to export\n",
    "    torch.randn(1, 16000),  # inputs of the model (waveform)\n",
    "    f'{saved_model_dir}/{timestamp}_frontend.onnx',  # filename of the ONNX model\n",
    "    input_names=['input'],  # input name in the ONNX model\n",
    "    dynamo=True,\n",
    "    optimize=True,\n",
    "    report=False,\n",
    "    external_data=False,\n",
    ")\n",
    "print(f\"‚úÖ Feature extractor saved: {saved_model_dir}/{timestamp}_frontend.onnx\")\n",
    "\n",
    "# Export Model to ONNX\n",
    "print(\"\\nüì¶ Exporting Model to ONNX...\")\n",
    "# Get a sample waveform from training dataset and extract features\n",
    "sample_waveform = train_dataset[0]['x'].squeeze(0).unsqueeze(0).cpu()  # (1, 16000)\n",
    "sample_features = feature_extractor_cpu(sample_waveform)  # (1, 1, n_mels, time)\n",
    "torch.onnx.export(\n",
    "    model_cpu,  # model to export\n",
    "    sample_features,  # inputs of the model (mel-spectrogram features)\n",
    "    f'{saved_model_dir}/{timestamp}_model.onnx',  # filename of the ONNX model\n",
    "    input_names=['input'],  # input name in the ONNX model\n",
    "    dynamo=True,\n",
    "    optimize=True,\n",
    "    report=False,\n",
    "    external_data=False,\n",
    ")\n",
    "print(f\"‚úÖ Model saved: {saved_model_dir}/{timestamp}_model.onnx\")\n",
    "\n",
    "# Check sizes\n",
    "fe_size = os.path.getsize(f'{saved_model_dir}/{timestamp}_frontend.onnx') / 1024\n",
    "model_size = os.path.getsize(f'{saved_model_dir}/{timestamp}_model.onnx') / 1024\n",
    "total_size = fe_size + model_size\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIZE REPORT (ONNX - Float32)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Feature Extractor: {fe_size:.2f} KB\")\n",
    "print(f\"Model: {model_size:.2f} KB\")\n",
    "print(f\"Total: {total_size:.2f} KB\")\n",
    "\n",
    "if total_size < 300:\n",
    "    print(\"‚úÖ PASSED: Total size < 300 KB (before quantization)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Size > 300 KB - quantization required!\")\n",
    "\n",
    "# Save Hyperparameters & Results\n",
    "print(\"\\nüìù Saving hyperparameters and results...\")\n",
    "output_dict = {\n",
    "    'timestamp': timestamp,\n",
    "    **CFG,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([output_dict])\n",
    "output_path = './keyword_spotter_results.csv'\n",
    "df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)\n",
    "print(f\"‚úÖ Results saved to {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "c0fbadf4c9c044908423f738834a5ee0",
  "kernelspec": {
   "display_name": "hw2-Xe6H3gEB-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
