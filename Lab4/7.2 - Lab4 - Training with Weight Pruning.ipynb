{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"d30abdad775a458cb9f1e1ca4e51a66a","deepnote_cell_type":"markdown"},"source":"# Training with Weight Pruning","block_group":"0ad78bf495244e34a5afb66553f5b33f"},{"cell_type":"markdown","metadata":{"cell_id":"97a268b4e38646848dead05d0150d04c","deepnote_cell_type":"markdown"},"source":"## Import the required modules","block_group":"a33e016ec44e440daa5ff625583d40fd"},{"cell_type":"code","metadata":{"source_hash":"43e38fac","execution_start":1763645600059,"execution_millis":3250,"execution_context_id":"6d14b056-0d58-430d-8bb2-8567aa4051c4","cell_id":"70c00fee8e0f4ce7beb556e57653488a","deepnote_cell_type":"code"},"source":"import numpy as np\nimport os\nimport pandas as pd\nimport random\nimport torch\nimport torchaudio\nimport torchaudio.transforms as T\nimport zipfile\n\nfrom time import time\nfrom torch import nn\nfrom torch.nn.utils import prune\nfrom torch.utils.data import Dataset\n\nfrom msc_dataset_lab3 import MSCDataset","block_group":"1545ff518efe4f0caf65a0d69e2ccc75","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"edf23c6a011b4ea5b143cc6fce4ea979","deepnote_cell_type":"markdown"},"source":"## Define the Hyperparameters","block_group":"cfcf98e23d1548c5a97ea70541aeac31"},{"cell_type":"code","metadata":{"source_hash":"f15d66bf","execution_start":1763645603359,"execution_millis":1,"execution_context_id":"6d14b056-0d58-430d-8bb2-8567aa4051c4","cell_id":"51fa3b06f97d442da2f7d5d32d08e8b3","deepnote_cell_type":"code"},"source":"CFG = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.04,\n    'frame_step_in_s': 0.02,\n    'n_mels': 40,\n    'f_min': 0,\n    'f_max': 8000,\n    'n_mfcc': 40,\n    'seed': 0,\n    'train_steps': 2000,\n    'train_batch_size': 32,\n    # Pruning hyperparameters:\n    'start_pruning': 499,     # start pruning at this training iteration\n    'end_pruning': 1499,      # stop pruning after this training iteration\n    'prune_amount': 0.1,      # percentage of connections to prune\n    'prune_every_steps': 100, # apply pruning every N steps\n}","block_group":"2e44ae8541294870b76dd8ef7e537125","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"7e7aca0f29c441cbb1846d5b9548bffc","deepnote_cell_type":"markdown"},"source":"## Define the target classes","block_group":"873b45ae10874d91a74c9de1543a9188"},{"cell_type":"code","metadata":{"source_hash":"3ed2c041","execution_start":1763645603409,"execution_millis":1,"execution_context_id":"6d14b056-0d58-430d-8bb2-8567aa4051c4","cell_id":"391218442bce45bf8db92cc85d8b3b6f","deepnote_cell_type":"code"},"source":"# TODO: Define the set of target classes\n# CLASSES = [...]","block_group":"c4341c125b404400b1191b7c0f1344f6","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"fe4e638c747d49cf85778b71adf79586","deepnote_cell_type":"markdown"},"source":"## Set Deterministic Behaviour","block_group":"31a042bdaaac4865b8736c306d6d1329"},{"cell_type":"code","metadata":{"source_hash":"f911a183","execution_start":1763645603469,"execution_millis":1,"execution_context_id":"6d14b056-0d58-430d-8bb2-8567aa4051c4","cell_id":"4dd225b422b14701b34cdc58dac0718c","deepnote_cell_type":"code"},"source":"torch.manual_seed(CFG['seed'])\nnp.random.seed(CFG['seed'])\nrandom.seed(CFG['seed'])","block_group":"18fde2c226164872a3936ffe1c6c296c","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"57631dc73a3a409db0ba7d79e2043900","deepnote_cell_type":"markdown"},"source":"## Create Datasets and Dataloaders for train/test","block_group":"a46d0a7152b34d1e89e9d9b7b667883d"},{"cell_type":"code","metadata":{"source_hash":"877f3d3a","execution_start":1763645603529,"execution_millis":67,"execution_context_id":"6d14b056-0d58-430d-8bb2-8567aa4051c4","cell_id":"de6901514f1b4aafbf5df837e720d530","deepnote_cell_type":"code"},"source":"transform = T.MFCC(\n    sample_rate=16000,\n    n_mfcc=CFG['n_mfcc'],\n    log_mels=True,\n    melkwargs=dict(\n        # Spectrogram parameters\n        n_fft=int(CFG['frame_length_in_s'] * CFG['sampling_rate']),\n        win_length=int(CFG['frame_length_in_s'] * CFG['sampling_rate']),\n        hop_length=int(CFG['frame_step_in_s'] * CFG['sampling_rate']),\n        center=False,\n        # Mel Spectrogram paramaters\n        f_min=CFG['f_min'],\n        f_max=CFG['f_max'],\n        n_mels=CFG['n_mels'],\n    )\n)\n\n# TODO: instantiate train_ds and test_ds objects\n# train_ds = ...\n# test_ds = ...\n\nsampler = torch.utils.data.RandomSampler(\n    train_ds,\n    replacement=True,\n    num_samples=CFG['train_steps'] * CFG['train_batch_size'],\n)\ntrain_loader = torch.utils.data.DataLoader(\n    train_ds,\n    batch_size=CFG['train_batch_size'],\n    sampler=sampler,\n    num_workers=2,\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_ds, batch_size=100, num_workers=2\n)","block_group":"d2a600696aee4c7bac1de6aea875ab30","execution_count":5,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"7224c7d9caa54a4498b2ab58e3db68f7","deepnote_cell_type":"markdown"},"source":"## Create the Model","block_group":"640209ebc3e04b718bfb4edc23cbd44d"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"db6697dd7e1742b28232e001e5117b83","deepnote_cell_type":"code"},"source":"# TODO: Write the model code\n# model = ...","block_group":"dbdd92791855429b9fb81b97fb6d8897","execution_count":5,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"86bef7a5405449acab420d5a380fcf84","deepnote_cell_type":"code"},"source":"print(model)","block_group":"c4907d92318c40c1b05a2a3de5c9db11","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"ce2c5b665e6e444e84651cab1b5cd56e","deepnote_cell_type":"markdown"},"source":"## Get parameters to prune","block_group":"803844adbd2c4221a70cb9ec9092b45c"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"efadfc1c022c4a22b9cfa59c2547c3c6","deepnote_cell_type":"code"},"source":"parameters_to_prune = [\n        (module, 'weight')\n        for module in model.modules()\n        if isinstance(module, (nn.Conv2d, nn.Linear))\n]\nparameters_to_prune","block_group":"b629332d220e45ee923237afdda4d42e","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"2780d9c752d7452b887c5c70ebbb216d","deepnote_cell_type":"markdown"},"source":"## Define function to inspect model sparsity","block_group":"8f68532a1cf14dfd8eb9b962ff49a0da"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"cd6596ac73fc4698b2e6a53edf03efdc","deepnote_cell_type":"code"},"source":"def print_sparsity(model):\n    total_zeros = 0\n    total_params = 0\n    print('\\nSparsity Report:')\n    for name, module in model.named_modules():\n        if isinstance(module, (nn.Conv2d, nn.Linear)):\n            if hasattr(module, 'weight_mask'):\n                # Before strip_pruning: use mask\n                mask = module.weight_mask\n                num_zeros = torch.sum(mask == 0).item()\n                num_params = mask.numel()\n            else:\n                # After strip_pruning: use weight\n                weight = module.weight\n                num_zeros = torch.sum(weight == 0).item()\n                num_params = weight.numel()\n\n            layer_sparsity = 100. * num_zeros / num_params\n            print(f'Layer {name:<30} | Sparsity: {layer_sparsity:.2f}%')\n            total_zeros += num_zeros\n            total_params += num_params\n\n    global_sparsity = 100. * total_zeros / total_params\n    print(f'Global Sparsity: {global_sparsity:.2f}%\\n')","block_group":"cd447c99d4dc4020a860e262cc631bb1","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"255cf4387d0d439abef70a9b765cc04e","deepnote_cell_type":"markdown"},"source":"## Define pruning control variables","block_group":"9472c4b2e1d5456cb92b735a52bf4f45"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"82b983535e764ba6ae03a73917158f18","deepnote_cell_type":"code"},"source":"start_pruning = CFG['start_pruning']\nend_pruning = CFG['end_pruning']\nprune_amount = CFG['prune_amount']\nprune_every_steps = CFG['prune_every_steps']","block_group":"2a4049113b944a54a6e691d54f0fbba8","execution_count":9,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"eece136cf7d644b5b5eadbb9d9065e5a","deepnote_cell_type":"markdown"},"source":"## Define the Training Loss and Optimizer","block_group":"3185381cfda54938a0427f433ad2e79e"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"f74ee52e76b444ecbce2a3f31b042c68","deepnote_cell_type":"code"},"source":"# TODO: instantiate the loss and optimizer objects\n# loss_module = ...\n# optimizer = ...","block_group":"0780d21f8f2f4cf4851efb68aa4bb4c8","execution_count":10,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"dccce16726e7470c8c529edabe5980b8","deepnote_cell_type":"markdown"},"source":"## Define the Training Loop with Iterative Weight Pruning\n\n### Pruning Neural Network\n\nThe most critical step for neural network pruning is to find out the unimportant synapse connections, i.e., weights, and set the weights to exactly zero. This step is also called pruning weights.\n\nPyTorch uses binary masks tensors to indicate which synapse connections, i.e., some weights, are not important and should be pruned. These binary masks are constant during neural network training or fine-tuning.\n\nThere are many ways to prune weights. Some straightforward methods use the magnitude of the weights to determine which weights are not important. For example, the `prune.L1Unstructured` method prunes parameters in a tensor by zeroing out the ones with the lowest L1-norm.\n\n### Sparsity for **Iterative** Pruning\n\nThe `prune.global_unstructured` function uses an `amount` argument which could be either the percentage of connections to prune (if it is a float between 0 and 1), or the absolute number of connections to prune (if it is a non-negative integer). When it is the percentage, it is the the relative percentage to the number of unpruned parameters in the module. For example, in **iterative** pruning, we prune the weights of a certain layer by `amount=0.2` in the first **iteration** and further prune the same layer by `amount=0.2` in the second **iteration**. The amount of the valid parameters after the pruning will be $1 \\times (1 - 0.2) \\times (1 - 0.2)$, and the sparsity of the parameters, i.e., the prune rate, in this module will be $1 - 1 \\times (1 - 0.2) \\times (1 - 0.2)$.\n\nFormally, the final prune rate could be calculated using the following equation. Suppose the relative prune rate for each **iteration** is $\\gamma$, the final prune rate, after $n$ **iterations**, will be\n\n$$1 - (1 - \\gamma)^n$$\n\nSimilarly, it is also easy to derive the final prune rate for the scenario that $\\gamma$ is different in each **iteration**.\n\n### One-Time VS Multi-Time Iterative Pruning + Fine-Tuning\n\nUnlike **one-time iterative pruning + fine-tuning** which achieves the desired prune rate by pruning and fine-tuning once, **multi-time iterative pruning + fine-tuning** achieves the desired prune rate by pruning and fine-tuning multiple-times. For example, to achieve the desired prune rate of 68.62%, we could run pruning and fine-tuning for 11 iterations, achieving prune rate of 10.00%, 19.00%, 27.10%, 34.39%, 40.95%, 46.86%, 52.17%, 56.95%, 61.26%, 65.13%, 68.62% in each iteration.\n\nUsually multi-time iterative pruning + fine-tuning is better than one-time iterative pruning + fine-tuning. \n\n### Local Pruning VS Global Pruning\n**Local pruning** is to prune the parameters module by module. The parameters from other modules do not affect the parameters being pruned. We could specify the prune rate for each layer in the network explicitly.\n\n**Global pruning** groups many different modules and prune the parameters in these modules as if they were from one module. Therefore, the prune rate for each individual layer will be different.\n\nTypically, global pruning performs much better than local pruning.","block_group":"2a31ed4159c44b97aabf1a924bc363fa"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"1ac6d5ca18bd4509a318734f405aae07","deepnote_cell_type":"code"},"source":"# Write the training loop code\nfor step, batch in enumerate(train_loader):\n    # TODO:\n    # ...\n    # loss = ...\n    # ...\n\n    # Iterative global pruning\n    if step >= start_pruning and step <= end_pruning:\n        if ((step + 1) % prune_every_steps) == 0:\n            prune.global_unstructured(\n                parameters_to_prune,\n                pruning_method=prune.L1Unstructured,\n                amount=prune_amount\n            )\n\n    if ((step + 1) % 100) == 0 or step == 0:\n        print(f'Step={step}; Training Loss={loss.item():.3f}')\n        print_sparsity(model)","block_group":"4661e9fbd5aa4eeab833c01793989832","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"a12f93af1bcd4d10a44c88c873d7a935","deepnote_cell_type":"markdown"},"source":"## Remove pruning re-parametrization\n\nOnce the fine-tuning is finished, the model weights can be finalized. We will combine the mask and weight together using the `prune.remove` method.","block_group":"bec2afa986b74a11832e4799493dc73a"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"7ac765750b98406e9aca9288f3e27899","deepnote_cell_type":"code"},"source":"for module, param_name in parameters_to_prune:\n    prune.remove(module, param_name)\n\nprint_sparsity(model)","block_group":"1576dc3beb3a449fae834206d8176b67","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"f08f1ab4a8df4122afe9549965b53322","deepnote_cell_type":"markdown"},"source":"## Evaluate the Model","block_group":"4e9febdd36e848068474d6bfca7d9917"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"f4ca4e0b532a40efa10b1b8d6170e45c","deepnote_cell_type":"code"},"source":"# Write the evaluation loop code\n# ...\n# test_accuracy = ...\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')","block_group":"226555cd63b748749f86f9f048e76d84","execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Test Accuracy: 90.25%\n"}],"outputs_reference":"dbtable:cell_outputs/eb817b1d-e578-47e5-a7cc-622ebf16985e","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"64a0488fe6dc415abbad86b837ada3f9","deepnote_cell_type":"markdown"},"source":"## Save the Model","block_group":"c7313010dc924e3a98c28bc09c109c11"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"984565781313439391e0e64c17efbaf9","deepnote_cell_type":"code"},"source":"timestamp = int(time())\n\nsaved_model_dir = './saved_models/'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\n\nprint(f'Model Timestamp: {timestamp}')\n\ntorch.onnx.export(\n    transform,  # model to export\n    torch.randn(1, 1, 16000),  # inputs of the model,\n    f'{saved_model_dir}/{timestamp}_frontend.onnx',  # filename of the ONNX model\n    input_names=['input'], # input name in the ONNX model\n    dynamo=True,\n    optimize=True,\n    report=False,\n    external_data=False,\n)\ntorch.onnx.export(\n    model,  # model to export\n    train_ds[0]['x'].unsqueeze(0),  # inputs of the model,\n    f'{saved_model_dir}/{timestamp}_model.onnx',  # filename of the ONNX model\n    input_names=['input'], # input name in the ONNX model\n    dynamo=True,\n    optimize=True,\n    report=False,\n    external_data=False,\n)","block_group":"c0e8426e78ac4cd49bbd16be4053e4d3","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"db4ee7d36a67435fac5483853cc01a0b","deepnote_cell_type":"markdown"},"source":"## Zip the Model\n\nEven after pruning is complete, the model's weights are still stored using dense matrix data structures. As a result, despite the sparsity introduced in the weight tensors, both the computational cost and storage requirements remain the same as those of the original dense model. To achieve latency improvements, specialized neural kernels for sparse matrix multiplication are required. For storage savings, lossless compression techniques like *zip* can be applied to encode the long sequences of zeros introduced by pruning into a more compact representation.","block_group":"5eab45ae13b24c188216a99616fbf9d3"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"5f3df77380b54d4dbe37702c8e1d4ed5","deepnote_cell_type":"code"},"source":"frontend_size = os.path.getsize(f'{saved_model_dir}/{timestamp}_frontend.onnx')\nmodel_size = os.path.getsize(f'{saved_model_dir}/{timestamp}_model.onnx')\ntotal_size = frontend_size + model_size\n\nwith zipfile.ZipFile(f'{saved_model_dir}/{timestamp}_model.onnx.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(f'{saved_model_dir}/{timestamp}_model.onnx')\n\nzip_model_size = os.path.getsize(f'{saved_model_dir}/{timestamp}_model.onnx.zip')\nzip_total_size = frontend_size + zip_model_size\n\nprint(f'Frontend Size: {frontend_size / 2**10:.1f}KB')\nprint(f'Model Size (ONNX): {model_size / 2**10:.1f}KB')\nprint(f'Total Size (ONNX): {total_size / 2**10:.1f}KB')\nprint()\nprint(f'Model Size (ZIP): {zip_model_size / 2**10:.1f}KB')\nprint(f'Total Size (ZIP): {zip_total_size / 2**10:.1f}KB')","block_group":"f1f583d312824c8dabf9af34398f4f21","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"2c8eebcda36f4cb99b1600317d41cccc","deepnote_cell_type":"markdown"},"source":"## Save Hyperparameters & Results","block_group":"86704fcaac854d209d08f69a18ac8e1d"},{"cell_type":"code","metadata":{"deepnote_to_be_reexecuted":true,"cell_id":"6e773744278e411bb416e23ebe3bf9dc","deepnote_cell_type":"code"},"source":"output_dict = {\n    'timestamp': timestamp,\n    **CFG,\n    'test_accuracy': test_accuracy\n}\n\ndf = pd.DataFrame([output_dict])\n\noutput_path='./mwp_results.csv'\ndf.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)","block_group":"dd0cef2db91e4f4fbf91c29e9a2b5d7a","execution_count":16,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3880e510-b64c-4bb5-b488-c2122d5d9e2d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"a6951edabd924cebab2a462f0db2ea63"}}